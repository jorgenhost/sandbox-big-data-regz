install.packages('bspm')
bspm::enable()
#install.packages('bspm')
bspm::enable()
#install.packages("DBI")
install.packages("duckdb")
install.packages('packages/dbreg_0.0.2.99.tar.gz', repos = NULL, type='source')
#install.packages('bspm')
bspm::enable()
#install.packages("DBI")
install.packages("duckdb")
install.packages('packages/dbreg_0.0.2.99.tar.gz', repos = NULL, type='source')
#library(duckdb)
library(dbreg)
library(duckdb)
install.packages("DBI")
install.packages("duckdb")
install.packages('packages/dbreg_0.0.2.99.tar.gz', repos = NULL, type='source')
#install.packages("DBI")
#install.packages("duckdb")
install.packages('packages/dbreg_0.0.2.99.tar.gz', repos = NULL, type='source')
library(duckdb)
library(duckdb)
install.packages('bspm')
bspm::enable()
install.packages('duckdb')  # Will now use binaries automatically
install.packages('bspm')
#install.packages('bspm')
bspm::enable()
install.packages('duckdb')  # Will now use binaries automatically
#install.packages("DBI")
install.packages("duckdb")
#install.packages("DBI")
#install.packages("duckdb")
install.packages('packages/dbreg_0.0.2.99.tar.gz', repos = NULL, type='source')
library(duckdb)
library(dbreg)
library(modelsummary)
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "model.rds")
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + factor(month, ref = 5) + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "model.rds")
print(res, fes =TRUE)
res = readRDS("model.rds")
# We are interested in the full coeftable
res$coeftable
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + relevel(factor(month), ref = "7") + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "model.rds")
res = readRDS("model.rds")
# We are interested in the full coeftable
res$coeftable
con <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")
# Describe the columns and types of the parquet table (read_parquet is a table function)
DBI::dbGetQuery(con, "DESCRIBE SELECT * FROM read_parquet('nyc-taxi/**/*.pq')")
con <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")
# Describe the columns and types of the parquet table (read_parquet is a table function)
DBI::dbGetQuery(con, "DESCRIBE SELECT * FROM read_parquet('temp.pq')")
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + relevel(factor(month), ref = "7") + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res = dbreg(tip_amount ~ fare_amount | week_x_weekday + passenger_cat + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res = dbreg(tip_amount ~ fare_amount | week_x_weekday + passenger_cat + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res$coeftable
res = dbreg(tip_amount ~ fare_amount | week_x_weekday + passenger_cat + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res$coeftable
res = dbreg(tip_amount ~ fare_amount | week_x_weekday + passenger_cat + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res$coeftable
res = dbreg(tip_amount ~ fare_amount | week_x_weekday + temp + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
res = dbreg(tip_amount ~ fare_amount | week_x_weekday + temp_col + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res$coeftable
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + week + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + week + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + week + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + week + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + week + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + week + VendorID,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + weekday,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res$coeftable
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + weekday,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res$coeftable
res = readRDS("model.rds")
# We are interested in the full coeftable
res$coeftable
res = dbreg(tip_amount ~ fare_amount + passenger_count | month + weekday,
path = "read_parquet('temp.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "modeltemp.rds")
res$coeftable
install.packages("DBI")
install.packages("duckdb")
install.packages("modelsummary")
install.packages('packages/dbreg_0.0.2.99.tar.gz', repos = NULL, type='source')
install.packages("Formula")
#install.packages("DBI")
#install.packages("duckdb")
#install.packages("modelsummary")
#install.packages("Formula")
install.packages('packages/dbreg_0.0.2.99.tar.gz', repos = NULL, type='source')
library(duckdb)
library(dbreg)
library(modelsummary)
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + relevel(factor(month), ref = "7") + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "model.rds")
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "model.rds")
res = readRDS("model.rds")
# We are interested in the full coeftable
res$coeftable
con <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")
# Describe the columns and types of the parquet table (read_parquet is a table function)
DBI::dbGetQuery(con, "DESCRIBE SELECT * FROM read_parquet('temp.pq')")
# Set environment variables early (before loading packages that spawn threads)
# Adjust the number "4" to suit your machine or CI limits.
Sys.setenv(
OMP_NUM_THREADS = "4",
OPENBLAS_NUM_THREADS = "4",
MKL_NUM_THREADS = "4",
NUMEXPR_MAX_THREADS = "4"
)
# Load packages after setting env vars
library(DBI)
library(duckdb)
# Set environment variables early (before loading packages that spawn threads)
# Adjust the number "4" to suit your machine or CI limits.
Sys.setenv(
OMP_NUM_THREADS = "4",
OPENBLAS_NUM_THREADS = "4",
MKL_NUM_THREADS = "4",
NUMEXPR_MAX_THREADS = "4"
)
# Load packages after setting env vars
library(DBI)
library(duckdb)
library(modelsummary)
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
# Load packages after setting env vars
library(DBI)
library(duckdb)
library(dbreg)
library(modelsummary)
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
duckdb::sql_exec("SET threads to 4")
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "model.rds")
# Load packages after setting env vars
library(DBI)
library(duckdb)
duckdb::sql_exec("SET threads to 4;")
library(dbreg)
library(modelsummary)
res = dbreg(tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
path = "read_parquet('nyc-taxi/**/*.pq')", ## path to hive-partitioned dataset
vcov = "hc1")
saveRDS(res, "model.rds")
con = dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
con = dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
dbExecute(con, "SET threads to 4;")
# create a 'taxi' table in our new nyc.db database from our parquet dataset
dbExecute(
con,
"
CREATE TABLE taxi AS
FROM read_parquet('nyc-taxi/**/*.parquet')
SELECT *
"
)
con = dbConnect(duckdb::duckdb(), dbdir = "databaz.db")
dbExecute(con, "SET threads to 4;")
# create a 'taxi' table in our new nyc.db database from our parquet dataset
dbExecute(
con,
"
CREATE TABLE taxi AS
FROM read_parquet('nyc-taxi/**/*.parquet')
SELECT *
"
)
# Load packages after setting env vars
library(DBI)
library(duckdb)
library(dbreg)
library(modelsummary)
con = dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
dbExecute(con, "SET threads to 4;")
# create a 'taxi' table in our new nyc.db database from our parquet dataset
dbExecute(
con,
"
CREATE TABLE taxi AS
FROM read_parquet('nyc-taxi/**/*.pq')
SELECT *
"
)
# now run our regression against this conn+table combo
dbreg(
tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
conn = con,     # database connection,
table = "taxi", # table name
vcov = "hc1"
)
con = dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
con = duckdb::dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
# 2. Set the thread limit using the package prefix
# This executes a command directly against the open connection 'con'
duckdb::dbExecute(con, "SET threads to 2;") # ✅ DuckDB is now limited to 2 threads
con = duckdb::dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
# 2. Set the thread limit using the package prefix
# This executes a command directly against the open connection 'con'
duckdb::dbExecute(con, "SET threads to 2;") # ✅ DuckDB is now limited to 2 threads
con = duckdb::dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
# 2. Set the thread limit using the package prefix
# This executes a command directly against the open connection 'con'
DBI::dbExecute(con, "SET threads to 2;") # ✅ DuckDB is now limited to 2 threads
# Load packages after setting env vars
library(DBI)
library(duckdb)
library(dbreg)
library(modelsummary)
con = dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
dbExecute(con, "SET threads to 2;")
# create a 'taxi' table in our new nyc.db database from our parquet dataset
dbExecute(
con,
"
CREATE TABLE taxi AS
FROM read_parquet('nyc-taxi/**/*.pq')
SELECT *
"
)
con = duckdb::dbConnect(duckdb::duckdb(), dbdir = "nyc-taxi/databaz.db")
# 2. Set the thread limit using the package prefix
# This executes a command directly against the open connection 'con'
DBI::dbExecute(con, "SET threads to 2;") # ✅ DuckDB is now limited to 2 threads
# Load packages after setting env vars
library(DBI)
library(duckdb)
library(dbreg)
library(modelsummary)
# create a 'taxi' table in our new nyc.db database from our parquet dataset
dbExecute(
con,
"
CREATE OR REPLACE VIEW taxi AS
SELECT *
FROM read_parquet('nyc-taxi/**/*.pq')
"
)
dbExecute(con, "DROP TABLE IF EXISTS taxi;")
# create a 'taxi' table in our new nyc.db database from our parquet dataset
dbExecute(
con,
"
CREATE OR REPLACE VIEW taxi AS
SELECT *
FROM read_parquet('nyc-taxi/**/*.pq')
"
)
# now run our regression against this conn+table combo
dbreg(
tip_amount ~ fare_amount + passenger_count | week_x_weekday + month + VendorID,
conn = con,     # database connection,
table = "taxi", # table name
vcov = "hc1"
)
